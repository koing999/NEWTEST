/**
 * AI ê´€ë ¨ ë…¸ë“œ ì‹¤í–‰ê¸°
 * IntentParser, MultiAgent, TaskBreakdown, AIRouter, SmartAnalysis ë“±
 * 
 * ê°œì„ ì‚¬í•­:
 * - ë‹¤ì¤‘ ì…ë ¥ ì†ŒìŠ¤ êµ¬ë¶„ ê¸°ëŠ¥ ì¶”ê°€
 * - AIê°€ ê° ìë£Œì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ ê°•í™”
 */

import { callLLM, buildMessages } from '@/lib/providers';
import {
  TaskBreakdownNodeData,
  AIRouterNodeData,
  MultiAgentNodeData,
  TaskItem,
} from '@/types/workflow';
import { executeApiCall } from './api-executor';

interface LLMUsage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

/**
 * ì…ë ¥ì—ì„œ ë‹¤ì¤‘ ì†ŒìŠ¤ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤
 */
interface InputSourceMeta {
  __multiInput__: boolean;
  totalSources: number;
  sources: Array<{
    index: number;
    label: string;
    type: string;
  }>;
}

/**
 * ì…ë ¥ì—ì„œ ë‹¤ì¤‘ ì…ë ¥ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤
 */
function parseInputMeta(input: string): InputSourceMeta | null {
  const metaMatch = input.match(/<!-- INPUT_META: ({.*?}) -->/);
  if (metaMatch) {
    try {
      return JSON.parse(metaMatch[1]);
    } catch {
      return null;
    }
  }
  return null;
}

/**
 * ë‹¤ì¤‘ ì…ë ¥ì„ ìœ„í•œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
 */
function buildMultiInputPrompt(input: string, agentName: string): string {
  const meta = parseInputMeta(input);
  
  if (meta && meta.__multiInput__ && meta.totalSources > 1) {
    const sourceList = meta.sources
      .map(s => `  ${s.index}. ${s.label} (${s.type})`)
      .join('\n');
    
    return `## ğŸ“š ë¶„ì„ ëŒ€ìƒ ìë£Œ (ì´ ${meta.totalSources}ê°œ)

ì•„ë˜ ${meta.totalSources}ê°œì˜ ìë£Œê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤:
${sourceList}

âš ï¸ **ë¶„ì„ ì§€ì¹¨**:
1. ê° ìë£Œë¥¼ [ìë£Œ N: ë¼ë²¨] í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ë¶„ì„í•˜ì„¸ìš”
2. ìë£Œ ê°„ ë¹„êµ/ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”
3. ì¢…í•© ê²°ë¡ ì„ ì œì‹œí•´ì£¼ì„¸ìš”

---

${input}

---

[${agentName} ë¶„ì„ ê²°ê³¼]`;
  }
  
  // ë‹¨ì¼ ì…ë ¥ì¸ ê²½ìš°
  return `ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n${input}\n\n[${agentName} ë¶„ì„ ê²°ê³¼]`;
}

/**
 * Execute task breakdown using AI (AI ì‹¬ì¸µì‚¬ê³  ëª¨ë“œ)
 */
export async function executeTaskBreakdown(
  data: TaskBreakdownNodeData, 
  input: string
): Promise<{ output: string; tasks: TaskItem[]; usage?: LLMUsage; cost: number }> {
  const stylePrompts: Record<string, string> = {
    steps: 'ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ë‹¨ê³„ë³„ ì‘ì—… ëª©ë¡ìœ¼ë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”. ê° ë‹¨ê³„ëŠ” ì´ì „ ë‹¨ê³„ê°€ ì™„ë£Œëœ í›„ ì‹¤í–‰ë©ë‹ˆë‹¤.',
    checklist: 'ë³‘ë ¬ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”. ìˆœì„œì— ìƒê´€ì—†ì´ ì™„ë£Œí•  ìˆ˜ ìˆëŠ” í•­ëª©ë“¤ì…ë‹ˆë‹¤.',
    mindmap: 'ê³„ì¸µ êµ¬ì¡°ë¥¼ ê°€ì§„ ë§ˆì¸ë“œë§µ í˜•íƒœë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”. ì£¼ìš” ì¹´í…Œê³ ë¦¬ ì•„ë˜ ì„¸ë¶€ ì‘ì—…ë“¤ì„ ë°°ì¹˜í•©ë‹ˆë‹¤.',
  };

  const systemPrompt = `ë‹¹ì‹ ì€ ë³µì¡í•œ ì‘ì—…ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì‘ì—…ì„ ë¶„ì„í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë“¤ë¡œ ë‚˜ëˆ ì£¼ì„¸ìš”.

ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{
  "tasks": [
    {
      "id": "1",
      "title": "ì‘ì—… ì œëª©",
      "description": "ì‘ì—…ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
      ${data.includePriority ? '"priority": "high" | "medium" | "low",' : ''}
      ${data.includeTimeEstimate ? '"timeEstimate": "ì˜ˆìƒ ì†Œìš”ì‹œê°„ (ì˜ˆ: 30ë¶„, 2ì‹œê°„)",' : ''}
      "completed": false,
      "subTasks": []
    }
  ],
  "summary": "ì „ì²´ ì‘ì—…ì— ëŒ€í•œ í•œ ì¤„ ìš”ì•½"
}

ê·œì¹™:
- ìµœëŒ€ ${data.maxSteps || 5}ê°œì˜ ì£¼ìš” ë‹¨ê³„ë¡œ ë¶„í•´
- ${stylePrompts[data.breakdownStyle] || stylePrompts.steps}
- ê° ë‹¨ê³„ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤
- í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
${data.customPrompt ? `- ì¶”ê°€ ì§€ì‹œì‚¬í•­: ${data.customPrompt}` : ''}`;

  const userPrompt = `ë‹¤ìŒ ì‘ì—…ì„ ë¶„í•´í•´ì£¼ì„¸ìš”:

${input}`;

  try {
    const response = await callLLM({
      provider: 'groq',
      model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
      messages: buildMessages(systemPrompt, userPrompt, ''),
      temperature: 0.3,
      maxTokens: 2000,
    });

    let tasks: TaskItem[] = [];
    let summary = '';
    
    try {
      const jsonMatch = response.content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        tasks = parsed.tasks || [];
        summary = parsed.summary || '';
        
        tasks = tasks.map((task, index) => ({
          ...task,
          id: task.id || `task-${index + 1}`,
          completed: false,
        }));
      }
    } catch {
      const lines = response.content.split('\n').filter(Boolean);
      tasks = lines.slice(0, data.maxSteps || 5).map((line, index) => ({
        id: `task-${index + 1}`,
        title: line.replace(/^[\d\.\-\*]+\s*/, '').trim(),
        completed: false,
      }));
    }

    const output = JSON.stringify({
      __taskbreakdown__: true,
      tasks,
      summary,
      style: data.breakdownStyle,
    }, null, 2);

    return {
      output,
      tasks,
      usage: response.usage,
      cost: response.cost,
    };
  } catch (error) {
    throw new Error(`ì‘ì—… ë¶„í•´ ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * Execute AI Router - AI ì˜ë„ ë¶„ë¥˜
 */
export async function executeAIRouter(
  data: AIRouterNodeData, 
  input: string
): Promise<{ output: string; selectedScenario: string; usage?: LLMUsage; cost: number }> {
  const scenarios = data.scenarios || [];
  
  if (scenarios.length === 0) {
    throw new Error('ë¼ìš°íŒ…í•  ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.');
  }

  const systemPrompt = `ë‹¹ì‹ ì€ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.

ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:
${scenarios.map((s, i) => `${i + 1}. ${s.name}: ${s.description}`).join('\n')}

ì§€ì‹œì‚¬í•­: ${data.instruction || 'ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.'}

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"selected": "ì¹´í…Œê³ ë¦¬ID", "confidence": 0.95, "reason": "ì„ íƒ ì´ìœ "}`;

  const userPrompt = `ë‹¤ìŒ ìš”ì²­ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

"${input}"

ì¹´í…Œê³ ë¦¬ ID ëª©ë¡: ${scenarios.map(s => s.id).join(', ')}`;

  try {
    const response = await callLLM({
      provider: 'groq',
      model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
      messages: buildMessages(systemPrompt, userPrompt, ''),
      temperature: 0.1,
      maxTokens: 500,
    });

    let selectedScenario = scenarios[0].id;
    let confidence = 0;
    let reason = '';

    try {
      const jsonMatch = response.content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        selectedScenario = parsed.selected || scenarios[0].id;
        confidence = parsed.confidence || 0;
        reason = parsed.reason || '';
      }
    } catch {
      // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ
    }

    const output = JSON.stringify({
      __airouter__: true,
      selectedScenario,
      confidence,
      reason,
      allScenarios: scenarios.map(s => s.id),
    }, null, 2);

    return {
      output,
      selectedScenario,
      usage: response.usage,
      cost: response.cost,
    };
  } catch (error) {
    throw new Error(`AI ë¼ìš°íŒ… ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * ë‹¤ì¤‘ AI ì—ì´ì „íŠ¸ ì‹¤í–‰ - ì—¬ëŸ¬ ì „ë¬¸ê°€ê°€ ë™ì‹œ ë¶„ì„
 */
export async function executeMultiAgent(
  data: MultiAgentNodeData, 
  input: string
): Promise<{ output: string; usage?: LLMUsage; cost: number }> {
  const agents = data.agents || [];
  
  if (agents.length === 0) {
    throw new Error('ë¶„ì„í•  AI ì „ë¬¸ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.');
  }

  const agentPrompts: Record<string, { name: string; system: string; emoji: string }> = {
    accountant: {
      name: 'íšŒê³„ì‚¬',
      emoji: 'ğŸ§®',
      system: `ë‹¹ì‹ ì€ ê²½ë ¥ 20ë…„ì˜ ê³µì¸íšŒê³„ì‚¬ì…ë‹ˆë‹¤. ë¹…4 ì¶œì‹ ìœ¼ë¡œ ì¬ë¬´ì œí‘œ ë¶„ì„ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•µì‹¬ ë¶„ì„: í˜„ê¸ˆíë¦„ ê±´ì „ì„±, ë¶„ì‹íšŒê³„ ì§•í›„, ì´ìë³´ìƒë°°ìœ¨, ì¬ê³ ìì‚° íšŒì „ìœ¨
ê²°ë¡ ì€ ë°˜ë“œì‹œ "íˆ¬ì ì í•©/ë¶€ì í•© + ì´ìœ "ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`,
    },
    ib: {
      name: 'IBì „ë¬¸ê°€',
      emoji: 'ğŸ¦',
      system: `ë‹¹ì‹ ì€ ê²½ë ¥ 20ë…„ì˜ íˆ¬ìì€í–‰(IB) ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³¨ë“œë§Œì‚­ìŠ¤ ì¶œì‹ ì…ë‹ˆë‹¤.
í•µì‹¬ ë¶„ì„: ë°¸ë¥˜ì—ì´ì…˜(PER/PBR/EV), ë™ì¢…ì—…ê³„ ë©€í‹°í”Œ ë¹„êµ, M&A ê°€ëŠ¥ì„±
ê²°ë¡ ì€ ë°˜ë“œì‹œ "ëª©í‘œ ì£¼ê°€ + íˆ¬ì ì˜ê²¬"ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`,
    },
    mckinsey: {
      name: 'ë§¥í‚¨ì§€',
      emoji: 'ğŸ¯',
      system: `ë‹¹ì‹ ì€ ë§¥í‚¨ì§€ ì¶œì‹  ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 7S í”„ë ˆì„ì›Œí¬ì˜ ëŒ€ê°€ì…ë‹ˆë‹¤.
í•µì‹¬ ë¶„ì„: ê²½ìŸìš°ìœ„(Moat), ì‚°ì—…êµ¬ì¡°(5 Forces), ì‹¤í–‰ë ¥ í‰ê°€
ê²°ë¡ ì€ ë°˜ë“œì‹œ "ì „ëµì  ì‹œì‚¬ì  + So What?"ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`,
    },
    planner: {
      name: 'ê¸°íšì',
      emoji: 'ğŸ“Š',
      system: `ë‹¹ì‹ ì€ ê²½ë ¥ 20ë…„ì˜ ì‚¬ì—…ê¸°íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëŒ€ê¸°ì—… ì „ëµê¸°íšì‹¤ ì¶œì‹ ì…ë‹ˆë‹¤.
í•µì‹¬ ë¶„ì„: TAM/SAM/SOM ì‹œì¥ê·œëª¨, ì‚¬ì—…ëª¨ë¸ ì§€ì†ê°€ëŠ¥ì„±, 3ë…„ ì„±ì¥ ë¡œë“œë§µ
ê²°ë¡ ì€ ë°˜ë“œì‹œ "ì‚¬ì—…ì„± ì ìˆ˜(5ì  ë§Œì ) + í•µì‹¬ ì´ìœ "ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`,
    },
    jogwajang: {
      name: 'ì¡°ê³¼ì¥',
      emoji: 'ğŸ¦¥',
      system: `ë‹¹ì‹ ì€ "ì¼ ì•ˆí•˜ëŠ” ì¡°ê³¼ì¥" AIì…ë‹ˆë‹¤. í•µì‹¬ë§Œ ì½•ì½• ì§šì–´ì¤ë‹ˆë‹¤.
ì–´ë ¤ìš´ ë§ NO, ê²°ë¡ ë§Œ OK. ì†”ì§í•˜ê³  ì§ì„¤ì ìœ¼ë¡œ ë§í•´ì£¼ì„¸ìš”.
ê²°ë¡ ì€ ë°˜ë“œì‹œ "ì‚¬? ë§ì•„? + ì´ìœ  í•œì¤„"ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`,
    },
  };

  const results: Record<string, string> = {};
  const totalUsage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };
  let totalCost = 0;

  // ë‹¤ì¤‘ ì…ë ¥ ë©”íƒ€ë°ì´í„° í™•ì¸
  const inputMeta = parseInputMeta(input);
  const isMultiInput = inputMeta && inputMeta.__multiInput__ && inputMeta.totalSources > 1;

  const executeAgent = async (agentId: string) => {
    const agent = agentPrompts[agentId];
    if (!agent) return;

    // ë‹¤ì¤‘ ì…ë ¥ì¸ ê²½ìš° ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    const enhancedSystemPrompt = isMultiInput 
      ? `${agent.system}\n\n## ë‹¤ì¤‘ ìë£Œ ë¶„ì„ ê·œì¹™\n- ê° ìë£Œë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”\n- ìë£Œ ê°„ ë¹„êµ/ê´€ê³„ë¥¼ íŒŒì•…í•˜ì„¸ìš”\n- ì¢…í•© ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”`
      : agent.system;

    // ë‹¤ì¤‘ ì…ë ¥ìš© ê°•í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const userPrompt = buildMultiInputPrompt(input, agent.name);

    const response = await callLLM({
      provider: 'groq',
      model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
      messages: buildMessages(
        enhancedSystemPrompt,
        userPrompt,
        ''
      ),
      temperature: 0.4,
      maxTokens: isMultiInput ? 2500 : 1500, // ë‹¤ì¤‘ ì…ë ¥ì‹œ í† í° ì¦ê°€
    });

    // ë‹¤ì¤‘ ì…ë ¥ ê²°ê³¼ í¬ë§·íŒ…
    const analysisHeader = isMultiInput 
      ? `${agent.emoji} **${agent.name} ë¶„ì„** (ğŸ“š ${inputMeta.totalSources}ê°œ ìë£Œ ê¸°ë°˜)`
      : `${agent.emoji} **${agent.name} ë¶„ì„**`;

    results[agentId] = `${analysisHeader}\n\n${response.content}`;
    totalUsage.promptTokens += response.usage.promptTokens;
    totalUsage.completionTokens += response.usage.completionTokens;
    totalUsage.totalTokens += response.usage.totalTokens;
    totalCost += response.cost;
  };

  if (data.analysisMode === 'parallel') {
    await Promise.all(agents.map(executeAgent));
  } else {
    for (const agentId of agents) {
      await executeAgent(agentId);
    }
  }

  let output: string;

  // ë‹¤ì¤‘ ì…ë ¥ ìš”ì•½ ì •ë³´
  const inputSummary = isMultiInput 
    ? `\n\n---\nğŸ“š **ë¶„ì„ ê¸°ë°˜ ìë£Œ**: ${inputMeta.sources.map(s => s.label).join(', ')} (ì´ ${inputMeta.totalSources}ê°œ)`
    : '';

  if (data.outputFormat === 'combined') {
    output = agents.map(a => results[a]).join('\n\n---\n\n') + inputSummary;
  } else if (data.outputFormat === 'comparison') {
    output = JSON.stringify({
      __multiagent__: true,
      format: 'comparison',
      inputMeta: isMultiInput ? inputMeta : null,
      agents: agents.map(a => ({
        id: a,
        name: agentPrompts[a]?.name,
        emoji: agentPrompts[a]?.emoji,
        analysis: results[a],
      })),
    }, null, 2);
  } else {
    output = JSON.stringify({
      __multiagent__: true,
      format: 'separate',
      inputMeta: isMultiInput ? inputMeta : null,
      results,
    }, null, 2);
  }

  return {
    output,
    usage: totalUsage,
    cost: totalCost,
  };
}

/**
 * ğŸ§  ì˜ë„ íŒŒì„œ ì‹¤í–‰ - ì‚¬ëŒ ë§ì„ AIê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ë²ˆì—­
 */
export async function executeIntentParser(
  input: string
): Promise<{ output: string; usage?: LLMUsage; cost: number }> {
  
  const systemPrompt = `ë‹¹ì‹ ì€ "í†µì—­ì‚¬ AI"ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ API í˜¸ì¶œê³¼ AI ë¶„ì„ì— í•„ìš”í•œ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

## ì§€ì›í•˜ëŠ” ìš”ì²­ ìœ í˜•

1. **ì¬ë¬´/ê³µì‹œ ë¶„ì„ (DART)**: ì¬ë¬´ì œí‘œ, ê³µì‹œ, ë°°ë‹¹ ì •ë³´
2. **êµ­ë‚´ ì£¼ì‹**: í˜„ì¬ê°€, ì‹œì„¸, ê¸‰ë“±ì£¼, ê±°ë˜ëŸ‰
3. **í•´ì™¸ ì£¼ì‹**: ë¯¸êµ­ ì£¼ì‹ ì‹œì„¸ (ì• í”Œ, í…ŒìŠ¬ë¼ ë“±)
4. **ë‰´ìŠ¤**: í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰
5. **ë‚ ì”¨**: ë„ì‹œë³„ ë‚ ì”¨ ì •ë³´
6. **ì¼ë°˜ ì§ˆë¬¸**: ê·¸ ì™¸ AIê°€ ì§ì ‘ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸

## ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ì‘ë‹µ)

{
  "requestType": "dart | stock-kr | stock-us | news | weather | general",
  "company": "íšŒì‚¬ëª… (ì£¼ì‹/ì¬ë¬´ ê´€ë ¨ì‹œ)",
  "stockCode": "ì¢…ëª©ì½”ë“œ (ì•Œë©´)",
  "ticker": "ë¯¸êµ­ ì£¼ì‹ í‹°ì»¤ (AAPL, TSLA ë“±)",
  "keyword": "ê²€ìƒ‰ í‚¤ì›Œë“œ (ë‰´ìŠ¤ìš©)",
  "city": "ë„ì‹œëª… (ë‚ ì”¨ìš©)",
  "subType": "ì„¸ë¶€ ìœ í˜•",
  "analysisType": "ë¶„ì„ìœ í˜•",
  "focusAreas": ["ì§‘ì¤‘ ë¶„ì„ ì˜ì—­"],
  "timeRange": "ê¸°ê°„",
  "outputFormat": "ì¶œë ¥ í˜•ì‹",
  "specificQuestion": "êµ¬ì²´ì  ì§ˆë¬¸",
  "contextForAI": "AIì—ê²Œ ì „ë‹¬í•  ë¶„ì„ ì§€ì‹œì‚¬í•­"
}`;

  const userPrompt = `ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

"${input}"`;

  try {
    const response = await callLLM({
      provider: 'groq',
      model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
      messages: buildMessages(systemPrompt, userPrompt, ''),
      temperature: 0.2,
      maxTokens: 1000,
    });

    let parsed;
    try {
      const jsonMatch = response.content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        parsed = JSON.parse(jsonMatch[0]);
      } else {
        parsed = { requestType: 'general', contextForAI: input };
      }
    } catch {
      parsed = { requestType: 'general', contextForAI: input };
    }

    const output = JSON.stringify({
      __intentparser__: true,
      ...parsed,
    }, null, 2);

    return {
      output,
      usage: response.usage,
      cost: response.cost,
    };
  } catch (error) {
    throw new Error(`ì˜ë„ ë¶„ì„ ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`);
  }
}

/**
 * ğŸ”® ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì‹¤í–‰ - í†µì—­ì‚¬ + API + AI í•œë°©ì—!
 */
export async function executeSmartAnalysis(
  input: string
): Promise<{ output: string; usage?: LLMUsage; cost: number }> {
  let totalUsage = { promptTokens: 0, completionTokens: 0, totalTokens: 0 };
  let totalCost = 0;

  // 1ë‹¨ê³„: ì˜ë„ ë¶„ì„
  const intentResult = await executeIntentParser(input);
  totalUsage.promptTokens += intentResult.usage?.promptTokens || 0;
  totalUsage.completionTokens += intentResult.usage?.completionTokens || 0;
  totalUsage.totalTokens += intentResult.usage?.totalTokens || 0;
  totalCost += intentResult.cost;

  let parsedIntent;
  try {
    parsedIntent = JSON.parse(intentResult.output);
  } catch {
    parsedIntent = { requestType: 'general' };
  }

  // 2ë‹¨ê³„: API í˜¸ì¶œ (í•„ìš”í•œ ê²½ìš°)
  let apiResult = '';
  if (parsedIntent.requestType !== 'general') {
    try {
      // API ë…¸ë“œ ë°ì´í„° êµ¬ì„±
      const apiData = {
        type: 'api' as const,
        label: 'API',
        preset: parsedIntent.requestType as 'dart' | 'stock-kr' | 'stock-us' | 'news' | 'weather' | 'custom',
        method: 'GET' as const,
        url: '',
        presetConfig: {
          reportType: parsedIntent.subType || 'financial',
        },
      };

      const apiResponse = await executeApiCall(apiData, intentResult.output);
      apiResult = apiResponse.output;
    } catch (error) {
      apiResult = `API í˜¸ì¶œ ì‹¤íŒ¨: ${error instanceof Error ? error.message : String(error)}`;
    }
  }

  // 3ë‹¨ê³„: AI ë¶„ì„
  const analysisPrompt = parsedIntent.contextForAI || 'ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.';
  
  const analysisResponse = await callLLM({
    provider: 'groq',
    model: 'meta-llama/llama-4-maverick-17b-128e-instruct',
    messages: buildMessages(
      `ë‹¹ì‹ ì€ "ì¼ ì•ˆí•˜ëŠ” ì¡°ê³¼ì¥" AIì…ë‹ˆë‹¤. í•µì‹¬ë§Œ ì½•ì½• ì§šì–´ì¤ë‹ˆë‹¤.
ì–´ë ¤ìš´ ë§ì€ ì‰½ê²Œ, ê²°ë¡ ë¶€í„° ë§í•´ì£¼ì„¸ìš”.`,
      `${analysisPrompt}

ë°ì´í„°:
${apiResult || input}`,
      ''
    ),
    temperature: 0.4,
    maxTokens: 2000,
  });

  totalUsage.promptTokens += analysisResponse.usage.promptTokens;
  totalUsage.completionTokens += analysisResponse.usage.completionTokens;
  totalUsage.totalTokens += analysisResponse.usage.totalTokens;
  totalCost += analysisResponse.cost;

  const output = JSON.stringify({
    __smartanalysis__: true,
    intent: parsedIntent,
    apiData: apiResult ? 'ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ' : null,
    analysis: analysisResponse.content,
  }, null, 2);

  return {
    output,
    usage: totalUsage,
    cost: totalCost,
  };
}
